title: 三重损失函数
type: post
date: 2018-01-04 10:27:55
category: 
edit: 2018-01-04 10:41:42
---

https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650735534&idx=3&sn=2a7c0d284e233f0aa7a46b457a41fe50&chksm=871ac7d0b06d4ec698ceed6ff3e126a420ada3b3d3df7f870e6f6f3184145a910586916eb5e3#rd

https://towardsdatascience.com/one-shot-learning-face-recognition-using-siamese-neural-network-a13dcf739e

这里介绍的文章包括了一个三重损失函数，这个在表示学习上应该已经有类似应用，我觉得非常好，适合我现在一些研究，以前都没注意到过，惭愧

a b 是一对
a c 不是一对
我们希望 distance(a, b) <= distance(a, c)
即 distance(a, b) - distance(a, c) <= -alpha
即 distance(a, b) - distance(a, c) + alpha <= 0

alpha本身是一个限制，也是空间，因为机器学习都是最小值优化，上式简单等价为 distance(a, b) - distance(a, c) + alpha = 0

如果损失函数如文中定为relu的话，如果alpha为0，那么也就是我们期望distance(a, b) - distance(a, c) = 0，但是实际上我们希望它们之间有距离，这个距离就是alpha（或者-alpha），也就是distance(a, b) - distance(a, c) = -alpha ，所以加上alpha这个超参。同时我们也没必要规定它们的距离必须非常非常大，足够大到alpha就行了，所以用relu，也就是只要距离超过alpha，就不提供反向传播的损失了。

这个应该是FaceNet这个论文提出的

FaceNet: A Unified Embedding for Face Recognition and Clustering

https://arxiv.org/pdf/1503.03832.pdf

DeepFace

https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf
