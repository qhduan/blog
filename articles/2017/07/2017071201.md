title: keras的precision，recall和F1
type: post
date: 2017-07-12 10:39:07
category: 
---


softmax版本的，但是也只是 ***2*** 分类的softmax，也就是输出两个Dense，0,1或者1,0

```python

def precision(y_true, y_pred):
    y_true = K.cast(K.argmax(y_true), 'float32')
    y_pred = K.cast(K.argmax(y_pred), 'float32')
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    # precision = true_positives / predicted_positives
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def recall(y_true, y_pred):
    y_true = K.cast(K.argmax(y_true), 'float32')
    y_pred = K.cast(K.argmax(y_pred), 'float32')
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    # recall = true_positives / possible_positives
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def fmeasure(y_true, y_pred):
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    return (p * r) / (p + r + K.epsilon()) * 2.0

```

普通版的，例如MSE，Binary Crossentropy之类的，也就是输出1个Dense，0到1，其实就是去掉了argmax


```

def precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    # precision = true_positives / predicted_positives
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def recall(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    # recall = true_positives / possible_positives
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def fmeasure(y_true, y_pred):
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    return (p * r) / (p + r + K.epsilon()) * 2.0

```