title: 强化学习与深度强化学习的思想
type: post
date: 2017-04-22 04:21:23
category: 
---

强化学习所学习的，是当我们处在某个环境下，我们应该做的最好的决策是什么？

假设我们所处的状态（State）是有限的，例如在开车的时候，前面红灯还是绿灯，前后左右有没有车，这些数据可以归结为有限多个状态。

假设前面有车没车，左边有车没车，前面是红灯还是绿灯，这样简单的归类，我们就有2x2x2 = 8种状态，状态数量可以任意多，有限即可（甚至对于无限个状态，我们也可以模糊的归结为有限）


每个状态下我们能做的事情（行动，action）是有限个的，还是开车，我们是直行、还是停下来等，这也就2种行动而已


在有限个状态下，有限个行动下，如果我们想在某种状态下选择一个最好的行动，并且如何学习到这种选择，就是强化学习的目标

---

例如：前方红灯，我们应该停车等待，也就是

| 状态/动作 | 直行 | 等待 |
| --- | --- |  --- |
| 红灯  | 0 | 1 |
| 绿灯  | 1 | 0 | 

归结为数学语言就是，在 状态 = 红灯 时，选择 行动 = 等待 的收益最大
绿灯时， 行动 = 直行 的收益最大

每一行是一种状态，每一列是一种行动，每个交叉点，是这个行动的收益。假设我们已经有了这张图表，我们也知道我们当前的状态（通过观察红绿灯），那么我们只要找到当前行，最大值的某个列的行动去做就好了。

所以问题就是，我们怎么得到这张表？

---

对于一个简单的问题，例如这张表只有10行，5列，50个元素，人类当然可以一个一个填写最优的可能性。不过假设状态有数千个，行动数量有数百个，我们就很难以人力完成这件工作。强化学习的一个简单思路，就是填完这张表。

它所希望的就是得到这个表格（Q-table），或者说Q function

在当前状态下，我们所应该选择的行动为：最大的 【Q（状态，行动） for 行动 in 所有可能的行动】

也就是在某个状态那一行，寻找哪个列（哪个行动）的值最大，值最大代表在这个状态下实行这个行动可以获取的奖励最大，或者说得到的收益最好。

---

强化学习不仅仅学习当前，也学习未来

我们可以计算出每个状态下，每个行动的收益（初始化一张Q-table）

这个初始的收益是不准确的，我们怎么更新它，我们使用：

当前行动收益  = 真实收益 + 当前行动做过之后下一个状态的最大收益

例如当前是红灯，我们直行的收益就很低（违反交规），这是当前的真实收益；但是如果我们是救护车，正在运病人，把病人快速运到的收益非常高（也就是闯红灯之后可能带来的收益），这个时候很可能是：我们应该闯红灯，因为未来远期的收益太高了。

这也是为什么强化学习需要学习远期收益

---

深度强化学习

深度强化学习是强化学习的深度学习版，其实并不高级。它使用一个神经网络来代替Q-table，因为状态可能太多了（虽然有限，但是数量太大），不过依然要求行动是有限的（例如几十种行动）

我们定义一个神经网络，输入是当前状态，输出是每个行动的收益，我们希望结果是训练出这个网络，每当我们输入一个状态获得每个行动的收益，然后我们可以从结果中选择收益最大的行动来直行，也就是代替Q-table的作用。